**Living Systematic Review of Research on Communicating Probability Information**

**Motivation**

Probabilistic forecast information is rapidly proliferating, injecting in a new wave of uncertainty into the forecast and warning process. Most scientists agree that this is a positive development, but incorporating probability information into risk communication can be challenging, as probabilities are notoriously difficult to communicate effectively to lay audiences. What does the research literature say about the best way to include probability information in risk communication? What is the evidence base for different communication practices? This project endeavors to address these questions by conducting a living review of relevant research from past studies and new studies as they become available.

**Systematic Review**

A systematic review is a type of literature review that uses a transparent and replicable methodology to identify relevant research from past studies, evaluate results from those studies, and synthesize findings both qualitatively and quantitatively. Historically, systematic reviews have been static; they synthesize the literature at a point in time and become out-of-date almost as soon as they are complete. To prevent this, living systematic reviews are beginning to replace static reviews. Living reviews follow the same steps but are updated as new research becomes available.

Systematic reviews generally include some combination of the following steps:

1. Define the study domain
2. Search for and identify relevant studies
3. Extract key topics, questions, methods, and findings from relevant studies
4. Evaluate the quality of relevant studies
5. Analyze and combine the studies to identify common topics, questions, methods, and findings

This review includes two more steps:

6. Assess common findings to develop recommendations to assist in communicating uncertainty and probabilities
7. Develop a living platform that incorporates new studies and relevant findings into the review as they become available

**Steps in This Review**

***1. Define the study domain.*** We focus on research studies that directly examine the impact of probability information on protective action decision making, intentions, and behaviors. Most of the studies in the review focus on the “best” or most effective way to communicate probability information. For example, they address questions like: are people more likely to take protective action when probability information is given verbally or numerically? Though extremely important, we do not include studies that indirectly examine these relationships by way of implication or suggestion. For example, we do not include studies that explore the relationship between numeracy and risk perceptions, which may have important implications for how people use probability information when making decisions.

***2. Search for and identify relevant studies.*** Consistent with best practices, we use three iterative methods to search for and identify relevant studies: (1) electronic search databases; (2) past literature reviews; and (3) citation chains. We use three electronic search databases (ProQuest, Web of Science, and EBSCO Academic Search Elite). At the start of the review, the electronic search databases gave us 1,559 potentially relevant studies; 725 of the studies were unique across the three databases. Of the 725 unique studies, 29 met the inclusion criteria---(1) they fit within the study domain (see above); (2) they report on new findings from a new research project (e.g., not a literature review, essay, or workshop report); and (3) they use a reasonably generalizable, transparent, and replicable methodology to conduct the research. Past literature reviews (12 of them) gave us 37 more studies that met the inclusion criteria and citation chains ("backwards" and "forwards") gave us 255 more studies that met the criteria, bring the beginning set of relevant studies to 327. Because this is a living systematic review, it is important to reiterate that this is only the beginning of the review. We plan to repeat these phases every few months to make sure that we are including the most up-to-date research. See *Bibliographic Archive* for a complete list of the articles in the review.

***3. Extract key topics, questions, methods, and findings from relevant studies.*** we carefully scan all relevant studies, extract key information, and store it in an electronic spreadsheet. In addition to basic bibliographic information, we note relevant research questions, independent (treatment) and outcome variables, research methodologies (i.e., survey experiment), information about research subjects, domains of study (i.e., weather, health, etc.), and primary findings.

***4. Evaluate the quality of relevant studies.*** We use three indicators of validity to assess quality: (1) external validity (are the results generalizable to the population of interest?); (2) internal validity (are we sure that variation in x causes variation in y?); and (3) domain validity (how relevant is the study domain to weather hazards and forecasting?). Each dimension is independently given a score by two researchers on a three point scale (1 = low; 2 = medium; 3 = high). We use the mean value of these scores to measure validity along each dimension and the overall validity of each study.

***5. Analyze and combine the studies to identify common topics, questions, methods, and findings.*** We use the database of key topics, questions, methods, and findings from each study to identify commonalities across the relevant studies. Broadly, most of the studies that are currently in the review ask one of two research questions: (1) How does probability information impact risk comprehension? (2) How does probability information impact protective action decisions, intentions, and/or behaviors? A significant majority of the studies use survey research and quantitative analysis (statistics) to address these questions. Many of the studies employ survey experiments to compare the impact of one type of probability information (e.g., a slight chance of rain) to the impact of an alternative way of the present the same information (e.g., a 20% chance of rain). We were able to identify 5 primary research topics and 14 secondary topics. See *State of the Literature* for more information on these topics. Across these topics, we were able to identify more than 100 unique research findings. See *Core Findings* for a complete list of these findings.

***6. Assess common findings to develop recommendations to assist in communicating uncertainty and probabilities.*** This is the most difficult and subjective step in the review. In this step, we work as a team to translate core findings into actionable recommendations. This is challenging because we have to decide which of the core findings are certain and relevant enough to support a recommendation. Relevancy is the primary challenge. Many studies in this review advance basic science but do not relate the findings to specific communication challenges. While valuable, it can be difficult to translate these findings into actionable advice for risk communicators. In addition, many studies in the review provide actionable advice on how to communicate probability in domains like healthcare. Again, the insight from these studies can be extremely valuable but difficult to translate into actionable advice for risk communicators who work in extreme weather/climate domains. This is where the subjectivity enters into the process. We pick the core findings that we believe are most relevant to forecasters.

***7. Develop a living platform that incorporates new studies and relevant findings into the review as they become available.*** Looks like you made it to the *ProbCom* platform---take a look and let us know if you have any questions!
